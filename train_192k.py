import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchaudio
import torchaudio.transforms as T
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast
from model import AudioUpscaleModel

# === –ù–ê–°–¢–†–û–ô–ö–ò AUDIOPHILE ===
TARGET_SR = 192000     # –ò—Ç–æ–≥: 192 –∫–ì—Ü
INPUT_SR = 32000       # –í—Ö–æ–¥: 32 –∫–ì—Ü
BATCH_SIZE = 2         # –ú–∞–ª–µ–Ω—å–∫–∏–π –±–∞—Ç—á —Ç.–∫. –æ—á–µ–Ω—å "—Ç—è–∂–µ–ª—ã–µ" –¥–∞–Ω–Ω—ã–µ
CHUNK_SEC = 1.0        # –ö–æ—Ä–æ—Ç–∫–∏–µ –∫—É—Å–∫–∏
NUM_EPOCHS = 10        # –≠–ø–æ—Ö–∏
LEARNING_RATE = 5e-5
MODEL_SAVE_PATH = "model_192k_beta.pth"
TRAIN_DIR = "/home/user/–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª/ResonAI/train/" # –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É

class AudiophileDegrader:
    """–ò–º–∏—Ç–∞—Ü–∏—è 32kHz, 16bit, ~192kbps -> —Ä–∞–∑–¥—É–≤–∞–µ–º –¥–æ 192kHz"""
    def __init__(self, device):
        self.down = T.Resample(TARGET_SR, INPUT_SR).to(device)
        self.up = T.Resample(INPUT_SR, TARGET_SR).to(device)
    
    def __call__(self, waveform):
        # 1. –†–µ–∂–µ–º –¥–æ 32 –∫–ì—Ü
        degraded = self.down(waveform)
        
        # 2. –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 16 –±–∏—Ç
        scale = 32767.0
        degraded = torch.round(degraded * scale) / scale
        
        # 3. –ò–º–∏—Ç–∞—Ü–∏—è 192 –∫–±–∏—Ç/—Å (–õ–µ–≥–∫–∏–π —à—É–º)
        noise = torch.randn_like(degraded) * 0.0015
        degraded = degraded + noise
        
        # 4. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ 192 –∫–ì—Ü (–∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è)
        return self.up(degraded)

class Dataset192k(Dataset):
    def __init__(self, root_dir):
        self.filepaths = []
        for root, _, files in os.walk(root_dir):
            for file in files:
                if file.lower().endswith((".flac", ".wav")):
                    self.filepaths.append(os.path.join(root, file))

    def __len__(self): return len(self.filepaths) * 2

    def __getitem__(self, idx):
        filepath = self.filepaths[idx % len(self.filepaths)]
        try:
            waveform, sr = torchaudio.load(filepath)
        except: return torch.zeros(2, int(TARGET_SR*CHUNK_SEC))

        if sr != TARGET_SR:
            resampler = T.Resample(sr, TARGET_SR)
            waveform = resampler(waveform)

        if waveform.shape[0] == 1: waveform = waveform.repeat(2, 1)
        waveform = waveform[:2, :]

        chunk_len = int(TARGET_SR * CHUNK_SEC)
        if waveform.shape[1] < chunk_len:
            waveform = torch.nn.functional.pad(waveform, (0, chunk_len - waveform.shape[1]))
        else:
            start = torch.randint(0, waveform.shape[1] - chunk_len, (1,)).item()
            waveform = waveform[:, start : start + chunk_len]

        return waveform

def train():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ Training Audiophile (32k->192k) on {device}")
    
    degrader = AudiophileDegrader(device)
    dataset = Dataset192k(TRAIN_DIR)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    
    model = AudioUpscaleModel().to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scaler = GradScaler()
    criterion = nn.L1Loss()

    for epoch in range(NUM_EPOCHS):
        model.train()
        for i, target_cpu in enumerate(dataloader):
            target = target_cpu.to(device)
            
            with torch.no_grad():
                inp = degrader(target)
            
            optimizer.zero_grad()
            with autocast():
                output = model(inp)
                loss = criterion(output, target)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            if i % 10 == 0: print(f"[Ep {epoch+1}] Step {i}, Loss: {loss.item():.5f}")

        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(f"üíæ Saved {MODEL_SAVE_PATH}")

if __name__ == "__main__":
    train()